
import { ExamDefinition } from './types';

export const PHASE_2_EXAM_1: ExamDefinition = {
    id: 'saa-c03-exam-1',
    title: 'AWS Solutions Architect Associate - Exam 1 (Real Practice)',
    code: 'SAA-C03',
    description: 'High-difficulty scenario-based questions standard focusing on VPC, S3 Strategy, ELB Troubleshooting, and Databases. Sourced from high-quality practice sets.',
    timeLimitMinutes: 45,
    passingScore: 720,
    totalQuestions: 20,
    questions: [
        {
            id: 'q1',
            scenario: 'A global enterprise is expanding its AWS footprint from a single region to multiple regions. Currently, they have 10 VPCs in the primary region connected via VPC Peering in a full-mesh topology. They plan to add 5 more VPCs in the primary region and 2 VPCs in each of the 4 new regions. The security team requires centralized inspection of all cross-VPC traffic using a fleet of third-party firewalls. What is the most architecturally efficient and scalable way to connect these VPCs while meeting the security requirements?',
            options: ['Continue using VPC Peering for all new VPCs and use Transit VPCs in each region with a mesh of VPN connections.', 'Implement a Transit Gateway in the primary region. Use Transit Gateway Peering to connect it to Transit Gateways in each of the other regions. Route all VPC traffic through a centralized "Inspection VPC".', 'Use AWS PrivateLink for all cross-VPC communication. This ensures traffic remains on the AWS backbone.', 'Configure a Hub-and-Spoke model using AWS Managed VPN between all VPCs.'],
            correctAnswer: 1,
            explanation: 'Transit Gateway (TGW) is the recommended solution for complex VPC connectivity at scale. TGW Peering allows for inter-region connectivity. Implementing an "Inspection VPC" allows for centralized traffic filtering.',
            distractorExplanation: 'VPC Peering mesh is unmanageable (N*(N-1)/2). PrivateLink is for service exposure, not general routing. VPN is less efficient/performant than TGW.',
        },
        {
            id: 'q2',
            scenario: 'A company has two VPCs (VPC-A and VPC-B) in the same region connected via VPC Peering. The administrator wants to add a third VPC (VPC-C) and peer it with VPC-A so that instances in VPC-B can communicate with VPC-C through VPC-A. Why will this configuration fail by default?',
            options: ['VPC Peering does not support CIDR blocks larger than /24.', 'VPC Peering is non-transitive. VPC-B cannot reach VPC-C via VPC-A.', 'You cannot have more than one peering connection per VPC.', 'Security groups do not allow traffic from "Peered" VPCs by default.'],
            correctAnswer: 1,
            explanation: 'AWS VPC Peering is explicitly non-transitive. If A peers with B and A peers with C, B and C cannot talk via A.',
            distractorExplanation: 'CIDR limits are loose. You can have many peers. SGs are not the fundamental blocking architect.',
        },
        {
            id: 'q3',
            scenario: 'A media company stores 500 TB of video archives on-premises. They need to move this data to Amazon S3. The data is rarely accessed but must be available within minutes if requested. The company wants the most cost-effective solution for both the transfer and the long-term storage. Which combination of services should they use?',
            options: ['Use AWS DataSync to move files directly over the internet to S3 Glacier Deep Archive.', 'Order an AWS Snowball Edge device to move the data, then use a lifecycle policy to move it to S3 Glacier Instant Retrieval.', 'Use AWS Storage Gateway (Volume Gateway) to cache the data on-premises and sync it to S3 Standard.', 'Use AWS Transfer Family (SFTP) to upload the data directly to S3 One Zone-IA.'],
            correctAnswer: 1,
            explanation: 'Snowball Edge is best for 500 TB offline transfer. Glacier Instant Retrieval provides millisecond access for rarely accessed data, cheaper than Standard-IA.',
            distractorExplanation: 'Deep Archive has 12h retrieval (fails "minutes"). DataSync over internet for 500TB is slow/expensive bandwidth. Standard is too expensive.',
        },
        {
            id: 'q4',
            scenario: 'An application generates log files that are moved to S3 Standard-IA after 30 days. Recently, costs spiked significantly despite the files being moved to Infrequent Access. You discover many files are under 128 KB. What is the cause?',
            options: ['S3 Standard-IA has a minimum storage duration of 90 days.', 'S3 Standard-IA has a minimum billable object size of 128 KB. Objects smaller than this are billed as if they are 128 KB.', 'S3 Lifecycle policies charge a transition fee for every 1,000 objects.', 'The logs should have been moved to S3 Intelligent-Tiering.'],
            correctAnswer: 1,
            explanation: 'S3 Standard-IA has a minimum billable object size of 128 KB. Storing 10KB files results in paying for 128KB, causing cost spikes for small files.',
            distractorExplanation: 'Duration is 30 days. Transition fees exist but the Size limit is the specific "small file" gotcha.',
        },
        {
            id: 'q5',
            scenario: 'An web app uses an ALB and ASG. Users report "502 Bad Gateway" errors during traffic spikes when scaling up. EC2 instances take 3 minutes to initialize software. What is the likely cause?',
            options: ['The ALB Health Check is too aggressive and marks the instances as healthy before the software is ready.', 'The ASG is terminating old instances before new ones are ready.', 'The target group deregistration delay is set to 0.', 'The security group doesn\'t allow traffic on the health check port.'],
            correctAnswer: 0,
            explanation: 'If the port opens (TCP health check) before the app is ready to serve (HTTP 502), the ALB sends traffic to a "Zombie" instance. Increase grace period or use deep health checks.',
            distractorExplanation: 'Scale-in issues don\'t cause errors on new instances. SG blocking would cause 503 Service Unavailable (no healthy hosts) or timeouts.',
        },
        {
            id: 'q6',
            scenario: 'An ASG scales based on CPU > 70%. Traffic spikes to 90% CPU for 10 minutes, but the ASG only adds 1 instance slowly, failing to handle the load. How can you make it more responsive?',
            options: ['Change to Step Scaling or use "Request Count Per Target" metric.', 'Increase the Max size of the ASG.', 'Decrease the Health Check Grace Period.', 'Use a larger instance type.'],
            correctAnswer: 0,
            explanation: 'Target Tracking is conservative. Step Scaling allows aggressive additions (e.g. +4 instances) when thresholds are breached significantly.',
            distractorExplanation: 'Max size is a ceiling, not a rate. Grace period is for startup. Larger instances change capacity, not responsiveness.',
        },
        {
            id: 'q7',
            scenario: 'A startup needs a MySQL-compatible database that can scale down to zero when not in use to save costs (unpredictable traffic). Which database meets this requirement?',
            options: ['RDS MySQL with Multi-AZ.', 'Aurora Serverless v2.', 'Aurora Serverless v1.', 'DynamoDB with On-Demand capacity.'],
            correctAnswer: 2,
            explanation: 'Aurora Serverless v1 is the only option that scales to ZERO (pauses). v2 has a minimum capacity (0.5 ACU).',
            distractorExplanation: 'RDS is provisioned. DynamoDB is NoSQL.',
        },
        {
            id: 'q8',
            scenario: 'An application requires a database that can handle millions of small, rapid read/write operations per second with sub-millisecond latency. Schema is flexible. Global replication required.',
            options: ['RDS for PostgreSQL with Read Replicas.', 'Amazon Aurora Global Database.', 'Amazon DynamoDB with Global Tables and DAX.', 'Amazon DocumentDB.'],
            correctAnswer: 2,
            explanation: 'DynamoDB + DAX provides microsecond latency at million-request scale. Global Tables handle replication. NoSQL suits flexible schema.',
            distractorExplanation: 'Relational DBs (RDS/Aurora) struggle with pure "millions of ops/flexible schema" requirements compared to Dynamo.',
        },
        {
            id: 'q9',
            scenario: 'A company requires a Disaster Recovery (DR) strategy with RTO < 15 mins and RPO < 5 mins. They want to minimize costs. Which strategy is best?',
            options: ['Backup and Restore.', 'Pilot Light.', 'Warm Standby.', 'Multi-Site Active-Active.'],
            correctAnswer: 1,
            explanation: 'Pilot Light (Data replicated, Compute off) meets RPO < 5 min via replication and RTO < 15 min via auto-scaling, while being cheaper than Warm Standby.',
            distractorExplanation: 'Backup/Restore is too slow (RTO > hours). Warm Standby is pricier (Compute running). Active-Active is most expensive.',
        },
        {
            id: 'q10',
            scenario: 'A mission-critical application runs in two regions. You need automatic failover if the primary region ALB is unreachable. Which Route 53 policy?',
            options: ['Weighted Routing.', 'Latency-Based Routing.', 'Failover Routing.', 'Multivalue Answer Routing.'],
            correctAnswer: 2,
            explanation: 'Failover Routing is designed for Primary-Secondary DR.',
            distractorExplanation: 'Weighted is for split traffic. Latency is for performance.',
        },
        {
            id: 'q11',
            scenario: 'An "Orders" service puts a message in SQS. "Billing" and "Shipping" services both need to receive the message independently. How to implement this Fan-out?',
            options: ['Two SQS queues, Orders writes to both.', 'One SNS Topic, two SQS queues subscribed to it.', 'Single SQS queue, both read.', 'Step Functions.'],
            correctAnswer: 1,
            explanation: 'The SNS-to-SQS fan-out pattern ensures decoupling. Orders publishes to SNS once; SNS delivers to both queues.',
            distractorExplanation: 'QS consumers compete for messages (visibility timeout), they don\'t both get it.',
        },
        {
            id: 'q12',
            scenario: 'SQS queue receives 1000 msg/sec. Consumer fleet (EC2) processes 500 msg/sec. Latency increasing. How to scale?',
            options: ['Increase VisibilityTimeout.', 'Use Long Polling.', 'Scale ASG based on `ApproximateNumberOfMessagesVisible`.', 'Use FIFO queues.'],
            correctAnswer: 2,
            explanation: 'Scaling based on Queue Depth is the standard pattern to handle backlog.',
            distractorExplanation: 'VisibilityTimeout helps with retry failures, not throughput. Long Polling saves empty receives.',
        },
        {
            id: 'q13',
            scenario: 'Trigger a Lambda when a JPG is uploaded to S3. Example actions (API call) might fail and need retries. What is the most reliable pattern?',
            options: ['S3 -> Lambda direct trigger.', 'S3 -> EventBridge -> Lambda.', 'S3 -> SQS -> Lambda.', 'CloudWatch Alarm -> Lambda.'],
            correctAnswer: 2,
            explanation: 'S3 -> SQS -> Lambda allows for buffering and robust retries (DLQ) if the downstream processing fails.',
            distractorExplanation: 'Direct triggers can lose events if retries are exhausted. SQS provides durability.',
        },
        {
            id: 'q14',
            scenario: 'Schedule a task every day at 3 AM. Writes report from RDS. Task takes 20 minutes.',
            options: ['EventBridge -> Lambda.', 'EventBridge -> Fargate.', 'Cron job on t3.micro.', 'Step Functions.'],
            correctAnswer: 1,
            explanation: 'Lambda has a 15-minute timeout. You must use ECS/Fargate for tasks > 15 mins.',
            distractorExplanation: 'See Lambda timeout.',
        },
        {
            id: 'q15',
            scenario: 'Company has huge on-prem files. Wants low latency access to most recent files from on-prem, but store all in AWS.',
            options: ['S3 File Gateway.', 'Volume Gateway - Stored.', 'Volume Gateway - Cached.', 'Tape Gateway.'],
            correctAnswer: 0,
            explanation: 'S3 File Gateway caches recent files locally while backing backend to S3. Best for "File" access.',
            distractorExplanation: 'Stored Volume keeps ALL data on prem. Cached Volume is block-based, usually for iSCSI apps, not general "Files".',
        },
        {
            id: 'q16',
            scenario: 'Backup large local iSCSI volumes to AWS. Full set of data must be available on-premises for immediate access (Limited bandwidth).',
            options: ['S3 File Gateway.', 'Volume Gateway - Stored.', 'Volume Gateway - Cached.', 'FSx File Gateway.'],
            correctAnswer: 1,
            explanation: 'Stored Volumes keep the primary data locally (Meeting the "Full set available" requirement) and snapshot to AWS.',
            distractorExplanation: 'Cached volumes only keep a cache locally.',
        },
        {
            id: 'q17',
            scenario: 'HPC application for Linux. Needs sub-millisecond latency and high throughput (hundreds of GB/s).',
            options: ['FSx for Windows.', 'FSx for Lustre.', 'FSx for ONTAP.', 'FSx for OpenZFS.'],
            correctAnswer: 1,
            explanation: 'FSx for Lustre is purpose-built for HPC and high-throughput workloads.',
            distractorExplanation: 'Windows is SMB. OpenZFS is general Linux but Lustre is the HPC beast.',
        },
        {
            id: 'q18',
            scenario: 'Migrating Windows file shares. Needs SMB, AD integration, and Shadow Copies.',
            options: ['EFS.', 'S3.', 'FSx for Windows File Server.', 'FSx for Lustre.'],
            correctAnswer: 2,
            explanation: 'FSx for Windows is the only native service with full Windows feature support (SMB, VSS, AD).',
            distractorExplanation: 'EFS is NFS.',
        },
        {
            id: 'q19',
            scenario: 'Migrate 10 TB Oracle DB to Aurora PostgreSQL. Downtime window 4 hours.',
            options: ['SCT + DMS (Full Load + CDC).', 'Export/Import CSV.', 'VM Import.', 'Oracle Data Pump.'],
            correctAnswer: 0,
            explanation: 'SCT converts schema. DMS moves data. CDC keeps it in sync until the cutover window.',
            distractorExplanation: 'CSV/Data Pump are too slow for limited downtime on 10TB.',
        },
        {
            id: 'q20',
            scenario: 'Sensitive PII data. Encrypt at rest. Key rotation every year. Minimize management.',
            options: ['KMS with Customer Managed Key (CMK) + Auto Rotation.', 'KMS with AWS Managed Key.', 'CloudHSM.', 'Client-side encryption.'],
            correctAnswer: 0,
            explanation: 'CMK Auto Rotation happens every 1 year. AWS Managed Keys rotate every 3 years.',
            distractorExplanation: 'AWS Managed Keys (3 years) fail the requirement.',
        }
    ]
};
